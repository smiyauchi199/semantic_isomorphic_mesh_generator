# 提案手法について
陰関数表現を用いて構造化モデルを生成する手法である．
目的は計算コストを抑えつつ，高精度な形状復元を行うことである．


## 取り組んだこと
* 提案手法では，Deep Implicit Templatesを利用し，ワーピング関数と反対の操作を行う逆ワーピング関数を新たに追加することで，構造化モデルを生成した．
* パラメータチューニング
    Deep Implicit Templates，形状変形ネットワークともにパラメータチューニングを行った．
* ネットワーク構造の変更
    形状変形ネットワークのネットワーク構造として，LSTMの代わりに多層パーセプトロンのみを用いた場合を検証したが，
    LSTMの方が精度が高かった．
* 損失関数の変更
    * Deep Implicit Templatesでは，ワーピング関数の変形自由度を持たせるために，LSTMの偶数番目の時刻のみを損失関数として計算する．これを全ての時刻を損失関数に含むよう変更したが，復元精度は変化しなかった．
    * [課題1] Deep Implicit Templatesのワーピング関数を[3DN](https://github.com/laughtervv/3DN)の非線形変換を行うネットワークに置き換えようと試みたが未実装のまま試せていない．
    * [課題2] ワーピング関数による写像時の自己交差発生を抑制するため，学習時の損失関数として[3DN](https://github.com/laughtervv/3DN)で提案されている`Local Permutation Invariant Loss`を追加したが自己交差の発生はなくならなかった．

* テンプレート空間における形状変形
    * [課題1] クラスごとに各物体形状がテンプレート形状と一致するよう，形状変形を行った．
        * [KeypointDeformer](https://github.com/tomasjakab/keypoint_deformer/)
            ネットワークに組み込まれている[Neural Cages](https://github.com/yifita/deep_cage)を用いた形状変形では，
            物体の細部の形状は保持したまま概形のみを変形するような方法であるため，テンプレート形状とは一致しなかった．
        * [3DN](https://github.com/laughtervv/3DN)
            3DNの元論文のように，非線形な形状変形は可能であるが，
            各物体形状が持つ特徴的な形状は保持されるためテンプレート形状とは一致しなかった．


## 今後の課題・未実施事項

### 課題1：物体形状がテンプレート形状と大きく異なる場合の形状復元精度低下
DITのワーピング関数はアフィン変換を行うため，各モデルの写像後形状とテンプレート形状は必ずしも一致しない．
これが構造化モデル生成時の形状復元精度低下につながってしまうため，全ての形状はテンプレート形状に一致することが望ましい．

#### アプローチ方法
* ワーピング関数を非線型写像関数に変更
    * free form deformation
    * 3D deformation

* クラスごとに複数のテンプレート形状を用意
例として，sofaの場合，Deep Implicit Templatesにより得られるテンプレート形状に加えて，
くの字型のsofaをテンプレート形状とする．

### 課題2：ワーピング時の自己交差の発生　
モデルの形状によっては，ワーピング関数による写像時の形状変形量が大きくなり，自己交差が発生してしまう．
対処法として3DNの損失関数を実装したが，改善しなかった．

### 課題3：ノイズに対する形状復元精度低下
入力点群がノイズを含む場合，特に存在割合が大きいときノイズの影響を大きく受け，形状復元精度が低下してしまう．

深層学習で陰関数表現を扱う際にノイズを考慮した手法を調査する必要がある．
